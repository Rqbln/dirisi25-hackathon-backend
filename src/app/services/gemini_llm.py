"""Wrapper LLM pour Google Gemini compatible avec LangChain."""

import os
from google import genai
from langchain_core.language_models.llms import LLM
from typing import Optional, List


class GeminiLLM(LLM):
    """LLM Google Gemini compatible avec LangChain."""

    model: str = "gemini-2.5-flash"
    temperature: float = 0.3
    max_tokens: int = 2048
    client: Optional[genai.Client] = None

    def model_post_init(self, __context):
        """Initialise le client Gemini apr√®s validation Pydantic."""
        # R√©cup√©rer la cl√© API depuis la variable d'environnement
        api_key = os.getenv("GOOGLE_API_KEY")

        if not api_key:
            raise ValueError(
                "GOOGLE_API_KEY non trouv√©e dans les variables d'environnement. "
                "Veuillez d√©finir votre cl√© API Gemini dans le fichier .env"
            )

        # Initialiser le client Gemini
        self.client = genai.Client(api_key=api_key)

        print(f"üîß GeminiLLM initialis√© - Mod√®le: {self.model}")

    @property
    def _llm_type(self) -> str:
        return "google_gemini"

    def _call(self, prompt: str, stop: Optional[List[str]] = None, **kwargs) -> str:
        """Appel au LLM Gemini."""
        try:
            # Configuration de g√©n√©ration
            generation_config = {
                "temperature": self.temperature,
                "max_output_tokens": self.max_tokens,
            }

            # Appel √† l'API Gemini
            response = self.client.models.generate_content(
                model=self.model, contents=prompt, config=generation_config
            )

            return response.text
        except Exception as e:
            return f"Erreur lors de l'appel √† l'API Gemini: {str(e)}"
